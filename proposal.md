# Final Proposal

> Please remember to rename each subtitle 

## Introduction and diagram for the Video Frame Interpolation project
When we watch movies and TV shows online today, most of them are 24fps format. As most of our screens like monitors or television are 60hz or even 120hz frame rate or more, use Figure 1 as example. We will see some common artifacts on the screen if we watch these videos on our screen. The reason for the artifacts occurring is because the low frame rate video will lose moving detail during the movement.

 ![Motion Interpolation](./pic1.png)

To avoid these common artifacts we want to create a program that is powered by Video Frame Interpolation technology to convert 24fps video to 60fps video. This is a technology that aims to generate non-existent frames in-between the original frames. The usage of this technology can be used not only frame rate up-conversion but also the slow-motion video. 

Triditional video frame interpolation methods are using the estimate optical flow to predict the movement of the object between input frames and synthesizing intermediate frames. However the performance will be vary depends the quality of optical flow. Futhermore, the optical flow methods still challenging to generate high-quality frames due to large motion and occlusions. 

![Different between triditional method and kernal based method](./pic4.png)

Since the goal of this project is to produce high-quality frame between existing frames, we decide to use the kernal based method to predict the frame. This method is to estimate spatially-adaptive convolution kernels for each output pixel and convolve the kernels with the input frames to generate a new frame. Specifically, for each pixel in the interpolated frame, the method takes two receptive field patches centered at that pixel as input and estimates a convolution kernel. The difference between these two method are shown as Figure 2.

As you can see in Figure 3, the object moves from one frame to the next frame. The model use kernal to draw the missing frame and then insert it in-between these two frames. The missing frame is generated by the CNN network with the other two frames as input.

 ![Generate frame with two frame](./pic3.jpg)

For the test part, we will use 60fps frames videos to train our model as shown in Figure 4. Each video in the data set will be process in three set of rames:t, t+1, t+2, where t is from 1th frames to 58 frames. We will use the t frame and t+2 frame as input and t+1 frame as ground truth. The output frame will be used to compare with the original t+1 frame to accurately model.

 ![Compare with the original frame](./pic2.jpg)

## How it is related to Deep Learning for CV ...  - Qiming 

Super-resolution is based on the concept proposed by the human visual system. The 1981 Nobel Prize winners David Hubel and Torsten Wiesel discovered that the information processing method of the human visual system is hierarchical. The first layer is the original data input. When a person sees a face image, they will first recognize the edges such as points and lines. Then enter the second layer, it will recognize some basic components in the image, such as eyes, ears, nose. Finally, an object model is generated, which is a complete face.

Convolutional neural network (CNN) in deep learning imitates the processing of the human visual system. Because of this, computer vision is one of the best application areas for deep learning. Super-resolution is a classic application in computer vision. Super-resolution is a method to improve image resolution through software or hardware methods. Its core idea is to exchange temporal bandwidth for spatial resolution. To put it simply, when I can't get an ultra-high-resolution image, I can take a few more images and then combine this series of low-resolution images into a high-resolution image. This process is called super-resolution reconstruction.

Video super-resolution technology is more complicated, not only needs to generate a detailed frame image but also maintain the continuity between images. To eliminate the frustration in the picture, intelligently generate interpolated frames, and reproduce 24 frames/second or 25 frames/second video to 60 frames/ second or 90 frames/second video. Most existing methods are supervised learning. For an original image and a target image, the mapping relationship between them is learned to obtain an enhanced image. However, such data sets are relatively few, and many are artificially adjusted, so self-supervised or weakly supervised methods are needed to solve this problem.

//not sure about mentioning self-supervised or weakly supervised methods
//besides CNN, what else needs to be mentioned in this section?


## Steps 
    - research paper 
    - dataset 
    - env/demo (implementation) 
    - results analasys
There are several steps towards making the project. First, We are going to read some related articles and look into previous works on video frame interpolation. We are currently working on bring tradition video coding algorithms into this project, and adapt them into machine learning algorithms. Then, we can decide which approach we are going to take to prediction inter-frame images.  
Second, we need to decide which dataset we are going to use to train and test the neural network. Since we plan to convert lower FPS videos to 60FPS or 90FPS, we need to find some native 60FPS and 90 FPS video or corresponding picture frames.  
In addition, we are going to implement our method and develop a demo for quantitative analysis and class presentation. At this moment, we decide to build the project using Keras library on Ubuntu 18.04.  
Finally, we will run own experiment demo againt exsting project/research, such as Sepconv Slomo. Mstrics including MSE, RMSE, PSNR, and SSIM will be used to quantify the results and evaluate the performance.


## Schedule
    - important dates
For the project schedule, we have planned the following dates and events at this moment. However, schedule may be slightly changed in the future according to the circumstances.
- April 23 (Thursday): Meeting and review on initial proposal
- April 27 (Monday): Initial proposal due
- April 30 (Thursday): Finish reading Chapter 6, 10, and 11 of the textbook. Daniel and Wang should finish reading the DAIN and Sepconv Slomo article and present it to the rest of the group
- May 4 (Monday): Meeting on final proposal 
- May 7 (Thursday): Meeting and review on final proposal
- May 11 (Monday): Final proposal due
- May 23 (Saturday): Final code review
- May 26 (Tuesday): Final report review
- May 30 (Saturday): Presentation rehearsal
- June 1 (Monday): Presentation


## Results and Metrics
    - results: 24/25 -> 60, -> 90
    - compared with other works  
### Results:
We use our interpolation model to process the input videos with frame rate 24/25 fps to generate new videos with increased frame rate 60 and 90 fps. And the input videos are from a wide variety of video datasets with different resolutions.

1. Vimeo90K

2. UCF101

3. HD

4. Middlebury dataset

### Evaluation and Metrics:
To evaluate our interpolation model, the metrics shown below will be used to perform the evaluation in quantitative and qualitative perspectives:

- Quantitative Evaluation:

  - model parameters: the total size of the parameters used in the model
  
  - runtime: the runtime of frame interpolation on different resolution in the unit of second
  
  - MSE: Mean squared error
  
  - PSNR: Peak signal-to-noise ratio, the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation.
  
  - SSIM: Structural similarity (SSIM) index is a method for predicting the perceived quality of various kinds of digital images and video. It is used for measuring the similarity between two images.
  
- Qualitative Evaluation:
  
  - Visual demonstration based on the image quality like the clear shape and the restored details.
  

Expected Outcomes:

  - a demo of playing the interpolated videos with higher frame rate while comparing against the original video
  - a report about our interpolation approach including its framework, implementation and evaluation

## Risks

In some cases the results generated by this method show the artifacts when the motion is larger than the size of the interpolation kernels. For example, the ghost effect could occur because the approach is not able to infer motion beyond the size of local kernels.

The training time could also take longer since it somehow depends on what hardware resources like GPU we would get for this project.

The graphics memory of GPU could be not enough for some data during training and testing.

## Reference

- Niklaus, S., Mai, L., & Liu, F. (2017). Video frame interpolation via adaptive separable convolution. In IEEE International Conference on Computer Vision (pp. 261-270).
- Bao, W., Lai, W. S., Ma, C., Zhang, X., Gao, Z., & Yang, M. H. (2019). Depth-aware video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3703-3712).
- Cote, R. (2016, February 22). Motion Interpolation On TVs: Soap Opera Effect. Retrieved April 26, 2020, from https://www.rtings.com/tv/tests/motion/motion-interpolation-soap-opera-effect.
- Epson Frame Interpolation. (n.d.). Retrieved April 26, 2020, from https://files.support.epson.com/docid/cpd5/cpd52094/source/adjustments/tasks/frame_interpolation.html.
- What Is The Soap Opera Effect? - Everything You Need To Know. (2019, June 10). Retrieved April 26, 2020, from https://www.displayninja.com/what-is-the-soap-opera-effect/.
